{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a05c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do the imports -- no need to change this\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys\n",
    "sys.path.insert(0, \"src/\")\n",
    "from environment import GridWorldEnvironment\n",
    "from MDPsolver import MDPsolver\n",
    "from utils import *\n",
    "from plot import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2665f",
   "metadata": {},
   "source": [
    "***Before starting, we recall the use of the gridworld environment.***\n",
    "\n",
    "The gridworld environment is instantiated via the class `GridWorldEnvironment`. \n",
    "\n",
    "***It takes 4 input values:***\n",
    "- `reward_mode` : Integer between 0 and 3 for different reward profiles,\n",
    "- `size`: Gridworld size,\n",
    "- `prop`: Probability assigned to the event that the agent does not follow the chosen action but another one selected uniformely at random,\n",
    "- `gamma`: Discount factor of the environment.\n",
    "\n",
    "***Interface of a Gridworld instance:***\n",
    "- `print(gridworld.n_states)` # return the number of states\n",
    "- `print(gridworld.n_actions)` # return the number of actions\n",
    "- `print(gridworld.r)` # return a matrix where each element indicates the reward corresponding to each (state, action) pair.\n",
    "- `print(gridworld.gamma)` # return the discount factor\n",
    "- `print(gridworld.sparseT[action])` # Input: action, Return: a matrix containing the state-to-state transition probabilities for the action passed as input.\n",
    "\n",
    "<img src=\"../dynamic_programming/src/vis_gridworld.png\" alt=\"fishy\" class=\"bg-primary\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2f35d",
   "metadata": {},
   "source": [
    "# Ex 1: Prove of the Policy Gradient Theorem via the Performance Difference Lemma (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20dd37",
   "metadata": {},
   "source": [
    "Denote $J(\\pi) = \\langle \\mu, V^\\pi \\rangle$ and recall that the performance difference lemma states\n",
    "$$\n",
    "J(\\pi) - J(\\pi') = \\mathbb{E}_{s \\sim \\lambda^{\\pi'}}[\\langle\\pi(\\cdot|s) - \\pi'(\\cdot| s) , Q^\\pi(s, \\cdot) \\rangle]\n",
    "$$\n",
    "where $\\lambda^{\\pi'} \\in \\Delta_{\\mathcal{S}\\times\\mathcal{A}}$ denotes the occupancy measure of the policy $\\pi'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a218d",
   "metadata": {},
   "source": [
    "Now let us consider direct parametization, and compute a partial derivative for the entry of $\\pi$ at index $(\\bar{s},\\bar{a})$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9740a2b",
   "metadata": {},
   "source": [
    "**Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb20f5",
   "metadata": {},
   "source": [
    "To help you compute this partial derivative, consider the policies $\\pi'$ parameterized by some (sufficiently small) $\\delta \\in \\mathbb{R}$ via\n",
    "$$\n",
    "    \\pi'(a|s) = \\begin{cases}\n",
    "        \\pi(\\bar{a}|\\bar{s}) + \\delta \\quad (\\text{if } (s,a)=(\\bar{s},\\bar{a}))\\\\\n",
    "        \\pi(a|s) \\quad (\\text{else})\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab6336",
   "metadata": {},
   "source": [
    "(1) Argue that\n",
    "$$\n",
    "\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} =\\lim_{\\delta \\rightarrow 0} \\frac{\\mathbb{E}_{s \\sim \\lambda^{\\pi'}}[\\langle\\pi(\\cdot|s) - \\pi'(\\cdot| s) , Q^\\pi(s, \\cdot) \\rangle]}{\\pi'(\\bar{a}|\\bar{s}) - \\pi(\\bar{a}|\\bar{s})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81b9f3",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03017f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76d3ee23",
   "metadata": {},
   "source": [
    "(2) Argue that $$\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} = \\lim_{\\delta \\rightarrow 0} \\lambda^{\\pi'}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a}).$$\n",
    "\n",
    "Hint: Write the expectation in the previous question as a sum and use the fact that $\\frac{\\pi(a|s) - \\pi'(a|s)}{\\pi(\\bar{a}|\\bar{s}) - \\pi'(\\bar{a}|\\bar{s})} = \\mathbf{1}_{\\{ (\\bar{s},\\bar{a}) = (s,a) \\}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39382c3",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b5001",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f075389",
   "metadata": {},
   "source": [
    "(3) Conclude that $$\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} = \\lambda^{\\pi}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a})$$\n",
    "for the direct parameterization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae1a9d",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92bec4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "343fdf2c",
   "metadata": {},
   "source": [
    "(4) Prove that for a general parametrization, it holds that\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta) = \\sum_{\\bar{s},\\bar{a}} \\lambda^{\\pi}(\\bar{s}, \\bar{a}) Q^\\pi(\\bar{s}, \\bar{a}) \\nabla_{\\theta} ( \\log \\pi_\\theta(\\bar{a}|\\bar{s}))\n",
    "$$\n",
    "\n",
    "Hint: Use the chain rule to write $$ \\nabla_\\theta J(\\pi_\\theta)  = \\sum_{\\bar{s},\\bar{a}} \\frac{\\partial J(\\pi)}{\\partial \\pi_\\theta(\\bar{a}|\\bar{s})} \\nabla_{\\theta} \\pi_\\theta(\\bar{a}|\\bar{s}), $$\n",
    "and then use the fact that $\\lambda^{\\pi}(\\bar{s},\\bar{a}) = \\lambda^{\\pi}(\\bar{s}) \\pi(\\bar{a}|\\bar{s})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c4b89",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8ae8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1fc0d1",
   "metadata": {},
   "source": [
    "# Ex 2: Natural Policy Gradient with softmax parameterization (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a963b",
   "metadata": {},
   "source": [
    "Recall that the iterates $\\{\\pi^t\\}^{\\infty}_{t=1}$ produced by NPG read as follows:\n",
    "$$\n",
    "\\pi^{t+1}(a|s) = \\frac{\\pi^t(a|s)e^{\\eta Q^{\\pi^t}(s,a) }}{\\sum_{a'} \\pi^t(a'|s) e^{\\eta Q^{\\pi^t}(s,a')}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ef509",
   "metadata": {},
   "source": [
    "**Question** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b69771",
   "metadata": {},
   "source": [
    "Implement NPG for an arbitrary step size $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(pi, env, tol=1e-10):\n",
    "    \"\"\"Implementation of policy evaluation through iteratively applying using a certain policy \n",
    "    Args:\n",
    "        pi: a policy stochastic passed with shape n_states times n_actions\n",
    "        env: environment\n",
    "        tol: a scalar to dermerminate whether the policy evaluation convergences\n",
    "    Returns:\n",
    "        v: an array with the values of the actions chosen\n",
    "        q: an array with the q values    \n",
    "    \"\"\"\n",
    "    v = np.zeros(env.n_states)\n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    while True:\n",
    "        v_old = np.copy(v)\n",
    "        for a in range(env.n_actions):\n",
    "            q[:, a] = env.r[:, a] + env.gamma * env.sparseT[a].dot(v)\n",
    "        for s in range(env.n_states):\n",
    "            v[s] = q[s].dot(pi[s])\n",
    "        if np.linalg.norm(v - v_old) < tol:\n",
    "            break\n",
    "    return v, q\n",
    "\n",
    "def npg_update(q, eta, old_policy):\n",
    "    \"\"\"Implementation of a greedy approach to choose policies (policy improvement)\n",
    "    Args:\n",
    "        q: q values obtained from evaluating the policies\n",
    "    Returns:\n",
    "        new_policy: the updates policy\n",
    "    \"\"\"\n",
    "    policy = np.zeros_like(q)\n",
    "    for s in range(q.shape[0]):\n",
    "        policy[s] = ??? # TODO (unnormalized update)\n",
    "        policy[s] = ??? # TODO: normalize\n",
    "    return policy\n",
    "\n",
    "def get_greedy_policy(q):\n",
    "    \"\"\"Implementation of a greedy approach to choose policies (policy improvement)\n",
    "    Args:\n",
    "        q: q values obtained from evaluating the policies\n",
    "    Returns:\n",
    "        policy: greedy policy (list)\n",
    "    \"\"\"\n",
    "    policy = np.zeros_like(q)\n",
    "    for s in range(q.shape[0]):\n",
    "        policy[s,np.argmax(q[s,:])] = 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003741c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NPG(env, eta): # apply NPG iterations for 30 steps\n",
    "    vs = []\n",
    "    policies = []\n",
    "    v = np.zeros(env.n_states)\n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    pi = np.ones_like(q)/env.n_actions\n",
    "    for k in range(30):\n",
    "        v_old = np.copy(v)\n",
    "        v, q = evaluate_policy(pi, env)\n",
    "        if eta < np.inf:\n",
    "            pi = npg_update(q, eta, pi)\n",
    "        else:\n",
    "            pi = get_greedy_policy(q)\n",
    "        vs.append(v)\n",
    "        policies.append(pi)\n",
    "    return vs, policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3848dd",
   "metadata": {},
   "source": [
    "Now, we run NPG for different stepsizes in the usual gridworld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mode = 2\n",
    "size = 10 \n",
    "prop = 0\n",
    "gamma=0.99\n",
    "gridworld = GridWorldEnvironment(reward_mode, size, prop=0, gamma=gamma)\n",
    "mu = np.ones(gridworld.n_states)/gridworld.n_states\n",
    "etas = [1e-3, 1e-2, 1e-1, 1, 100, 1e7, np.inf]\n",
    "v_different_etas = []\n",
    "pi_different_etas = []\n",
    "for eta in etas:\n",
    "    values_pi, policies = NPG(gridworld, eta=eta)\n",
    "    v_different_etas.append(values_pi)\n",
    "    pi_different_etas.append(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34421f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = MDPsolver(gridworld)\n",
    "solver.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: if this plot appears with a too large legend, rerun this line once more\n",
    "plot_log_lines([np.array([mu.dot(solver.v - v) for v in v_different_etas[i]]) for i, _ in enumerate(etas)], [f\"Subopt for eta {eta}\" for eta in etas], [\"Iteration\", \"Subopt\"], \"figs\", \"NPG.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a32b5",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce2d48",
   "metadata": {},
   "source": [
    "Show that NPG with $\\eta = \\infty$ coincides with Policy Iteration (PI).\n",
    "\n",
    "More formally: Assuming that $a^\\star_s := \\mathrm{argmax}_a Q^{\\pi^t}(s,a)$ is unique for all $s$, prove that $$ \\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a|s)e^{\\eta Q^{\\pi^t}(s,a) }}{\\sum_{a'} \\pi^t(a'|s) e^{\\eta Q^{\\pi^t}(s,a')}} = \\begin{cases} 1 \\quad \\text{if} \\quad a = a^\\star_s \\\\ 0 \\quad \\text{otherwise} \\end{cases},$$\n",
    "and explain how this relates to PI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8170e",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a7126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d927c2",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74dd8a",
   "metadata": {},
   "source": [
    "Is this observation in line with the empirical results in the plot above? I.e., is the plot for $\\eta = \\infty$ as you would expect it for PI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a00c6",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ee997",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ecc1863",
   "metadata": {},
   "source": [
    "# Ex 2.1 Slow Changing Property of NPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13376b5f",
   "metadata": {},
   "source": [
    "In this exercise you will investigate by how much consecutive iterates $\\pi^t$ and $\\pi^{t+1}$ produced by NPG differ and how this distance is controlled by the step size $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd4b77",
   "metadata": {},
   "source": [
    "Plot $$\\max_{s \\in \\mathcal{S}} || \\pi^{t+1}(a|s) - \\pi^t(a|s) ||_1$$ for different values of $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_variation(policies):\n",
    "    variation = []\n",
    "    for pi, pip in zip(policies[1:], policies[:-1]):\n",
    "        variation.append(np.max([ ??? for s in range(pi.shape[0])])) # TODO\n",
    "    return variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines(np.array([ compute_policy_variation(np.array(pi_different_etas)[i])\n",
    "                           for i, _ in enumerate(etas)]), \n",
    "               [f\" eta = {eta}\" for eta in etas], \n",
    "               [\"Iteration\", \"Variation\"], \"figs\", \"NPG.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0b5d5",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22525a76",
   "metadata": {},
   "source": [
    "Empirically, is the largest change (among all iterations) between consecutive iterations is larger for smaller or large values of $\\eta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e839b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e62fa81b",
   "metadata": {},
   "source": [
    "## Some Theory to Motivate the Observation Above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3cf2d",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Our goal is to prove that $$ || \\pi^{t+1}(\\cdot|s) - \\pi^t(\\cdot|s) ||_1 \\leq \\frac{\\eta}{1 - \\gamma} \\quad \\forall s \\in \\mathcal{S}, \\forall t \\in [T].$$\n",
    "\n",
    "We guide you towards this result by breaking the proof into small steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24f01a",
   "metadata": {},
   "source": [
    "1) Prove that $$ \\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] - \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^t(a'|s) \\exp (\\eta Q^{\\pi^t}(s,a'))\\bigg) $$\n",
    "\n",
    "Hint: First apply Pinkser's inequality https://en.wikipedia.org/wiki/Pinsker%27s_inequality to prove that $$\\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq KL(\\pi^{t+1}(s)||\\pi^t(s)), $$ then plug in the formula for $\\pi^{t+1}$ into the KL term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d76b91",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61921e0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e49b4a0e",
   "metadata": {},
   "source": [
    "2) Prove that \n",
    "$$\n",
    "\\sum_{a\\in \\mathcal{A}} \\pi^{t+1}(a|s) \\exp(- \\eta Q^{\\pi^t}(s,a)) = \\frac{1}{\\sum_{a'\\in \\mathcal{A}} \\pi^t(a|s) \\exp(\\eta Q^{\\pi^t}(s,a) )}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be47fbb",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56f638",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "986fa85b",
   "metadata": {},
   "source": [
    "3) Using the results in 1) and 2) prove that \n",
    "\n",
    "$$ \\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] + \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^{t+1}(a'|s) \\exp (-\\eta Q^{\\pi^t}(s,a'))\\bigg). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f5ea0",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902403f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "361f7f1d",
   "metadata": {},
   "source": [
    "4) Using Hoeffding's Lemma https://en.wikipedia.org/wiki/Hoeffding%27s_lemma (on the sum in the log term!) and the fact that $$-\\frac{1}{1-\\gamma} \\leq Q^{\\pi^t}(s,a) \\leq \\frac{1}{1-\\gamma},$$ conclude that \n",
    "$$\\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\frac{\\eta^2}{2 (1 - \\gamma)^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2621f87",
   "metadata": {},
   "source": [
    "# Ex 3: OPPO: The importance of Exploration in Policy Gradient (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86788fb",
   "metadata": {},
   "source": [
    "In this exercise, we will investigate how crucial it is to perform exploration. That is, adding bonuses to avoid suffering the mismatch coefficients in the convergence bounds.\n",
    "\n",
    "Let us recall that the standard sample based version of NPG suffers the mismatch coeffcients in the bounds (see Slide 22 in Lecture 5). Those are avoided by OPPO ( See slide 30 in Lecture 5 ).\n",
    "\n",
    "**To see clearly the advatange of OPPO we will consider an MDP with unbounded mismatch coefficients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032cbad",
   "metadata": {},
   "source": [
    "**Question: example of unbounded mismatch coefficients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df78d50",
   "metadata": {},
   "source": [
    "Consider a 10 x 10 gridworld, the initial state is always the bottom right corner, i.e. the initial distribution $\\mu$ equals $1$ at this starting state and it is zero everywhere else. Can you compute a finite bound for \n",
    "$$\\max_\\pi \\max_{s \\in \\mathcal{S}} \\bigg |\\frac{\\lambda^\\pi(s)}{\\mu(s)} \\bigg|,$$\n",
    "i.e. the mismatch coefficient? If not, argue for which reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fa3b9",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a194249",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "607852c8",
   "metadata": {},
   "source": [
    "In the following, we experiment with OPPO with and without bonuses in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mode = 0\n",
    "size = 10\n",
    "gamma=0.999\n",
    "gridworld = GridWorldEnvironment(reward_mode, size, prop=0, gamma=gamma)\n",
    "r_max = np.max(gridworld.r)\n",
    "r_min = np.min(gridworld.r)\n",
    "gridworld.r = (gridworld.r - r_min) / (r_max - r_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oppo(K: int = 10000, H: int = 20, beta: float = 0.0001, eta=10000) -> List[float]:\n",
    "    \"\"\"\n",
    "    Function implementing OPPO with UCB bonuses algorithm.\n",
    "\n",
    "    :param K: Number of episodes, positive int\n",
    "    :param H: Number of steps per episode, positive int\n",
    "    :param beta: Algorithm hyperparameter, constant which scales the bonuses, positive float\n",
    "\n",
    "    :return: reward after each step, list of K * H floats\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize tabular records\n",
    "    rewards = []\n",
    "    Q = H * np.ones((H, gridworld.n_states, gridworld.n_actions))\n",
    "    V = H * np.ones((H + 1, gridworld.n_states))\n",
    "    policy = H * np.ones((H, gridworld.n_states, gridworld.n_actions))/gridworld.n_actions\n",
    "    V[H, :] = 0\n",
    "    estimated_transitions = np.ones((H, gridworld.n_states, \n",
    "                                     gridworld.n_actions, \n",
    "                                     gridworld.n_states))/gridworld.n_states\n",
    "    N = np.zeros((H, gridworld.n_states, gridworld.n_actions))\n",
    "    N_next = np.zeros((H, gridworld.n_states, gridworld.n_actions, gridworld.n_states))\n",
    "\n",
    "    for k in range(K):  # Episode loop\n",
    "        state = 99  # Initial state\n",
    "        for h in range(H):  # Step loop\n",
    "            #NPG Update\n",
    "            policy[h, state, :] = ??? # TODO (unnormalized)\n",
    "            policy[h, state, :] /= ??? # TODO: normalize\n",
    "            # Sample one action the current policy\n",
    "            a = np.random.choice(gridworld.n_actions, p=???) # TODO\n",
    "            rewards.append(gridworld.r[state, a])\n",
    "\n",
    "            # Record that we visited this state-action pair (again)\n",
    "            N[h, state, a] += ??? # TODO\n",
    "\n",
    "            # Get the new state according to the transition dynamics\n",
    "            new_state = np.random.choice(gridworld.n_states,\n",
    "                                         p=gridworld.T[a][state])\n",
    "            N_next[h, state, a, new_state] += ??? # TODO\n",
    "            \n",
    "            estimated_transitions[h,state,a,:] = ??? # TODO\n",
    "            \n",
    "\n",
    "            # Calculate the UCB bonus\n",
    "            bonus = beta * np.sqrt(H ** 3 / N[h, state, a])\n",
    "\n",
    "            # Update Q according to the algorithm\n",
    "            Q[h, state, a] = np.clip(???, 0, H) # TODO\n",
    "\n",
    "            # Update V as the Q-value of the optimal actions for the current state\n",
    "            V[h, state] = ??? # TODO\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340404be",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = []\n",
    "betas = [0,1e-5, 1e-4, 1e-3, 1e-1]\n",
    "\n",
    "for beta in betas:\n",
    "    reward_OPPO = oppo(beta = beta)  # You can play around with the arguments if you like\n",
    "    to_plot.append(np.cumsum(reward_OPPO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ f\"OPPO beta = {beta}\" for beta in betas]\n",
    "plot_lines(\n",
    "    to_plot,\n",
    "    labels,\n",
    "    [\"Iteration\", \"Reward collected so far\"],\n",
    "    \"figs\",\n",
    "    \"ucbvseps\",\n",
    "    show=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c551415",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f8834",
   "metadata": {},
   "source": [
    "Why does setting $\\beta = 0$ lead to bad results? \n",
    "\n",
    "*Hint: Explain using the remarks in slide 28 and the theoretical bound in Slide 22 of Lecture 5*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c806d",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469ffed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "468763e4",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acaeed3",
   "metadata": {},
   "source": [
    "Why does setting $\\beta$ too large lead to poor results?\n",
    "\n",
    "*Hint: Answer using the regret bound for OPPO given at the beginning of slide 30.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8018b",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125a7fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "290ab81f",
   "metadata": {},
   "source": [
    "# Ex 4: REINFORCE with parametrized policies (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RJjcHYbq4u0b",
   "metadata": {
    "id": "RJjcHYbq4u0b"
   },
   "source": [
    "In this exercise, we will investigate the effect of choosing different baselines in the reinforce implementation.\n",
    "This topic is covered from Slide 31 on in Lecture 5.\n",
    "\n",
    "**Hint: You may want to use Google Colab to run the experiments faster, but you don't have to.**\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8691d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you may need to run this to make sure to have the correct versions\n",
    "!pip install gym==0.25.2\n",
    "!pip install gym-notices==0.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ALUGQk5x4u0b",
   "metadata": {
    "executionInfo": {
     "elapsed": 6543,
     "status": "ok",
     "timestamp": 1710702571987,
     "user": {
      "displayName": "Adrian Müller",
      "userId": "01347970328935212551"
     },
     "user_tz": -60
    },
    "id": "ALUGQk5x4u0b"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCUCh_HY4u0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1710702571987,
     "user": {
      "displayName": "Adrian Müller",
      "userId": "01347970328935212551"
     },
     "user_tz": -60
    },
    "id": "yCUCh_HY4u0d",
    "outputId": "2de5cdc9-50e3-4bc8-ff81-4e7ce808f69d"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5H7mDuS4u0e",
   "metadata": {
    "id": "e5H7mDuS4u0e"
   },
   "source": [
    "### Instantiate the Environment and Agent\n",
    "\n",
    "The CartPole environment is very simple. It has discrete action space (2) and 4 dimensional state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SSoX_CHo4u0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1710702589197,
     "user": {
      "displayName": "Adrian Müller",
      "userId": "01347970328935212551"
     },
     "user_tz": -60
    },
    "id": "SSoX_CHo4u0e",
    "outputId": "2c2d3f67-949d-4950-b098-4ff6ed9eff59"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module): # definie the policy network\n",
    "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1) # we just consider 1 dimensional probability of action\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf43e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE (with reward-to-go)\n",
    "# --> with gradient estimator according to version 2 of the PG theorem (not using Q-values, but reward to go)\n",
    "def reinforce_rwd2go(policy, optimizer, early_stop=False, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        rewards_to_go = [sum([discounts[j]*rewards[j+t] for j in range(len(rewards)-t) ]) for t in range(len(rewards))]\n",
    "\n",
    "        # Calculate the loss\n",
    "        policy_loss = []\n",
    "        for i in range(len(saved_log_probs)):\n",
    "            log_prob = saved_log_probs[i]\n",
    "            G = rewards_to_go[i]\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if early_stop and np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9338c",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "1. Find **two** good baselines that improve over the implementation of REINFORCE without baseline. You should plot their results below.\n",
    "\n",
    "You can take inspiration from the Example Notebook we attached for lecture 4, but you **cannot use exactly the same**.\n",
    "\n",
    "2. Explain why you chose your baselines and why you think they are reasonable.\n",
    "\n",
    "*Note:* You may also change other parameters such as the learning rate, as long as you clearly state it in your response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882949ca",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b1ab9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb926e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_baseline(state): # Example Baseline from lecture 4 (for inspiration)\n",
    "  angle = state[2]\n",
    "  value = 100*(0.25-angle**2) # TO BE CHANGED USING YOUR BASELINE\n",
    "  return value\n",
    "\n",
    "def baseline_1(state): # TO BE CHANGED USING YOUR BASELINE 1\n",
    "  ???   # TODO\n",
    "  return ??? # TODO\n",
    "\n",
    "def baseline_2(state): # TO BE CHANGED USING YOUR BASELINE 2\n",
    "  ???   # TODO\n",
    "  return ??? # TODO\n",
    "\n",
    "# PLOT 3: reward-to-go with baseline REINFORCE\n",
    "# --> with gradient estimator according to version 3 of the PG theorem (not using Q-values, but reward to go)\n",
    "# --> here, we consider only fixed (handcrafted) baseline functions b : S -> R; clearly, training a NN to predict V^{\\pi}(s) as a baseline is also possible (and interesting!)\n",
    "def reinforce_rwd2go_baseline(policy, optimizer, early_stop=False, baseline=naive_baseline, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        baseline_values = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            baseline_values.append(baseline(state))\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        rewards_to_go = [sum([discounts[j]*rewards[j+t] for j in range(len(rewards)-t) ]) for t in range(len(rewards))]\n",
    "\n",
    "        # Calculate the loss\n",
    "        policy_loss = []\n",
    "        for i in range(len(saved_log_probs)):\n",
    "            log_prob = saved_log_probs[i]\n",
    "            G_centered = rewards_to_go[i] - baseline_values[i]\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            policy_loss.append(-log_prob * G_centered)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if early_stop and np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# PLOT 1: run REINFORCE\n",
    "policy_rwd2go = Policy().to(device)\n",
    "optimizer_rwd2go = optim.Adam(policy_rwd2go.parameters(), lr=1e-2)\n",
    "scores_rwd2go = reinforce_rwd2go(policy_rwd2go, optimizer_rwd2go, early_stop=False, n_episodes=2000)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# PLOT 2: run REINFORCE and YOUR baseline 1\n",
    "policy_baseline_1 = Policy().to(device)\n",
    "optimizer_baseline_1 = optim.Adam(policy_baseline_1.parameters(), lr=1e-2)\n",
    "scores_baseline_1 = reinforce_rwd2go_baseline(policy_baseline_1, optimizer_baseline_1, baseline=baseline_1, early_stop=False, n_episodes=2000)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# PLOT 3: run REINFORCE and YOUR baseline 2\n",
    "policy_baseline_2 = Policy().to(device)\n",
    "optimizer_baseline_2 = optim.Adam(policy_baseline_2.parameters(), lr=1e-2)\n",
    "scores_baseline_2 = reinforce_rwd2go_baseline(policy_baseline_2, optimizer_baseline_2, baseline=baseline_2, early_stop=False, n_episodes=2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the learning progress\n",
    "\n",
    "# Create the plot\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the scores with specified colors and labels\n",
    "ax.plot(np.arange(1, len(scores_rwd2go) + 1), scores_rwd2go, color='green', label='No Baseline')\n",
    "ax.plot(np.arange(1, len(scores_baseline_1) + 1), scores_baseline_1, color='blue', label='Baseline 1')\n",
    "ax.plot(np.arange(1, len(scores_baseline_2) + 1), scores_baseline_2, color='red', label='Baseline 2')\n",
    "\n",
    "# Set the labels with a larger font size\n",
    "ax.set_ylabel('Total reward (= time balanced)', fontsize=20)\n",
    "ax.set_xlabel('Episode #', fontsize=20)\n",
    "\n",
    "# Set the tick labels to a larger font size\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "# Add a legend with a specified font size\n",
    "ax.legend(fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
