{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf87d12",
   "metadata": {},
   "source": [
    "# Linear Programming exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do the imports -- no need to change this\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys\n",
    "sys.path.insert(0, \"src/\")\n",
    "from environment import GridWorldEnvironment\n",
    "from MDPsolver import MDPsolver\n",
    "from utils import *\n",
    "from plot import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853b76d",
   "metadata": {},
   "source": [
    "***Before starting, we recall the use of the gridworld environment.***\n",
    "\n",
    "The gridworld environment is instantiated via the class `GridWorldEnvironment`. \n",
    "\n",
    "***It takes 4 input values:***\n",
    "- `reward_mode` : Integer between 0 and 3 for different reward profiles,\n",
    "- `size`: Gridworld size,\n",
    "- `prop`: Probability assigned to the event that the agent does not follow the chosen action but another one selected uniformely at random,\n",
    "- `gamma`: Discount factor of the environment.\n",
    "\n",
    "***Interface of a Gridworld instance:***\n",
    "- `print(gridworld.n_states)` # return the number of states\n",
    "- `print(gridworld.n_actions)` # return the number of actions\n",
    "- `print(gridworld.r)` # return a matrix where each element indicates the reward corresponding to each (state, action) pair.\n",
    "- `print(gridworld.gamma)` # return the discount factor\n",
    "- `print(gridworld.sparseT[action])` # Input: action, Return: a matrix containing the state-to-state transition probabilities for the action passed as input.\n",
    "\n",
    "<img src=\"../dynamic_programming/src/vis_gridworld.png\" alt=\"fishy\" class=\"bg-primary\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ed7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mode = 2 # see below to visualize the reward function\n",
    "size = 3 # size of the gridworld (3x3)\n",
    "prop = 0.1 # proportion of randomly taken steps due to noise in the environment \n",
    "gamma=0.99 # discount factor\n",
    "\n",
    "gridworld = GridWorldEnvironment(reward_mode, size, prop=0, gamma=gamma)\n",
    "print('Reward function: \\n', gridworld.r) # visualize the reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c00def",
   "metadata": {},
   "source": [
    "We will use a solver to compute the **optimal value function** to measure the suboptimality of the policies produced by our algorithms.\n",
    "\n",
    "To access the optimal value function use `solver.v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = MDPsolver(gridworld) # call the MDPsolver class\n",
    "solver.value_iteration() # call the value iteration method to find V^* once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcae6be",
   "metadata": {},
   "source": [
    "## Ex 1: Solving the Primal [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327f0a2",
   "metadata": {},
   "source": [
    "We will leverage the powerful `scipy` library to solve **Linear Programs (LPs)**. Below we give an example of using the [linprog module](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html) to solve a simple LP:\n",
    "\n",
    "\\begin{aligned}\n",
    " & \\min_{x := \\begin{bmatrix}\n",
    "x_0  \\\\\n",
    "x_1\n",
    "\\end{bmatrix}  \\in \\mathbb{R}^2}  \\begin{bmatrix}\n",
    "-1  \\\\\n",
    "4\n",
    "\\end{bmatrix} ^\\top x\n",
    "\\\\&  \\text{s.t.}  \\begin{bmatrix}\n",
    "-3 & 1 \\\\\n",
    "1 & 2 \n",
    "\\end{bmatrix} \\mathbb{x} \\le  \\begin{bmatrix}\n",
    "6  \\\\\n",
    "4 \n",
    "\\end{bmatrix} , \\quad  x_1\\ge -3 \\,.\n",
    "\\end{aligned}\n",
    "\n",
    "The **code example below will be instructive** when you later construct the LP corresponding to the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog # import the linear programming solver from scipy\n",
    "\n",
    "# Note that the bounds argument in linprog is a  sequence of (min, max) pairs for each element in x, defining the minimum and maximum values.\n",
    "x0_bounds = (None, None)\n",
    "x1_bounds = (-3, None)\n",
    "\n",
    "# call linear programming solver:\n",
    "res = linprog([-1, 4], \n",
    "              A_ub=[[-3, 1], [1, 2]], \n",
    "              b_ub=[6, 4], \n",
    "              bounds=[x0_bounds, x1_bounds],\n",
    "              method=\"simplex\")\n",
    "print('The solution is ', res.x) # access and print the solution\n",
    "print(res.message) # print the message of the solver (e.g., terminated successfully)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88733424",
   "metadata": {},
   "source": [
    "Now we are ready to use `linprog` to solve the LP problem in gridworld.\n",
    "\n",
    "We define a distribution `mu` over the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.ones((gridworld.n_states,1))/gridworld.n_states # initialize mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de463e84",
   "metadata": {},
   "source": [
    "Next, we need to **instantiate the constraints of the primal problem**. Notice that `scipy` requires the inequality constraints in the form $A_{ub} x \\leq b_{ub}$.\n",
    "\n",
    "\n",
    "To this end, we rewrite the constraint $EV \\geq \\gamma P V + r$ in the form expected by `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(env):\n",
    "    E = np.kron(np.eye(env.n_states), np.ones(env.n_actions)).T # build the matrix E\n",
    "    P = env.T.transpose((1,0,2)).reshape(env.n_states*env.n_actions, -1) # build the matrix P\n",
    "    return env.gamma*P - E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f95df",
   "metadata": {},
   "source": [
    "(Side remark: The indexing in the matrices `E` and `P` here is slightly different from the one in the lecture. This does not change any of the formulas from the slides since both matrices are indexed in the same way, consistently throughout this exercise. You may ignore this comment.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e8803",
   "metadata": {},
   "source": [
    "Then, use the function `linprog` of `scipy` to solve the dual problem and show that you can retrieve the same $V^\\star$ computed with the solver and stored in `solver.v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call linear programming solver with the correct inputs:\n",
    "primal_out = linprog(???, \n",
    "                     A_ub=???, \n",
    "                     b_ub= ???,\n",
    "                     method=\"simplex\",\n",
    "                     bounds=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812b1e9",
   "metadata": {},
   "source": [
    "(*Hint:* Check slide 23 for the matrix form (or slide 8), lecture 3 to recall the primal.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb37bce",
   "metadata": {},
   "source": [
    "We can access the solution of the primal LP using `primal_out.x`. Compare the error with the value of `solver.v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(primal_out.x - solver.v) # plot 2-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39bed8",
   "metadata": {},
   "source": [
    "***Question***\n",
    "\n",
    "By running the following two cells, plot (in matrix form) `mu` and the difference between `solver.v` and `primal_out.x`. Then, answer the following question: Can we ensure that `np.linalg.norm(primal_out.x - solver.v)` is zero (up to numerical errors) for the current value of `mu`?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(mu.reshape(size, size), cmap='PuBu_r', vmin=0,vmax=1) # plot distribution mu over states \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa96200",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow((solver.v - primal_out.x).reshape(size, size), cmap='PuBu_r') # plot the difference between the value function and the solution of the linear program\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31ed2c",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d7807",
   "metadata": {},
   "source": [
    "Next, we repeat the same experiment with a different value of `mu`. \n",
    "\n",
    "Note that we do **not** overwrite `mu` or `primal_out` (but instead have `mu2` and `primal_out2`, since for all other exercises, we will use the original, uniform `mu`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite mu with a new distribution:\n",
    "mu2 = np.zeros((gridworld.n_states,1))\n",
    "mu2[-3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a01ad8",
   "metadata": {},
   "source": [
    "Fill in the following, as before but for the distribution `mu2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the LP with the new mu2:\n",
    "primal_out2 = linprog(???, \n",
    "                      A_ub=???, \n",
    "                      b_ub=???,\n",
    "                      method=\"simplex\",\n",
    "                      bounds=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d82f4e",
   "metadata": {},
   "source": [
    "We compare with the value computed by Value Iteration `solver.v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(primal_out2.x - solver.v) # plot 2-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af0f46",
   "metadata": {},
   "source": [
    "Note that now, the optimal solution of the primal LP `primal_out2.x` and the optimal value function `solver.v` do not coincide anymore! \n",
    "\n",
    "This is despite the fact that the optimal value function `solver.v` was computed **right in the beginning**, before we even initialized the `mu` for solving the LP the first time. So `solver.v` is definitely the correct optimal value function for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49811cdb",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Why does it make sense that the two values do not coincide?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37cf64",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec069475",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfc16b",
   "metadata": {},
   "source": [
    "***Question***\n",
    "\n",
    "We plot again in matrix form `mu2`, `solver.v` and `primal_out2.x`. Then answer the following: What can happen in the states where `mu2 = 0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(mu2.reshape(size, size), cmap='PuBu_r', vmin=0,vmax=1) # plot distribution mu over states\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow((solver.v - primal_out2.x).reshape(size, size), cmap='PuBu_r') # plot the difference between the value function and the solution of the linear program\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b8475",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632636c",
   "metadata": {},
   "source": [
    "# Ex 2: Solving the Dual [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15361f21",
   "metadata": {},
   "source": [
    "Try now to solve the dual using again the `linprog` routine of scipy. \n",
    "\n",
    "Note that we did **not** overwrite `mu` (i.e. it is still uniform, and `primal_out.x` is the primal solution for this `mu`).\n",
    "\n",
    "(*Hint:* Check slide 23 for the matrix form (or slide 11), lecture 3 to recall the dual.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the dual LP with the correct inputs:\n",
    "dual_out = linprog(???, \n",
    "              A_eq= ???, \n",
    "              b_eq= -(1 - gridworld.gamma)*mu,\n",
    "              method=\"simplex\",\n",
    "              bounds=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc02c2",
   "metadata": {},
   "source": [
    "We verify that **strong duality** holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3759aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_value = dual_out.x.dot(gridworld.r.reshape(gridworld.n_states*gridworld.n_actions)) # compute the optimal value of the dual\n",
    "primal_value = (1 - gridworld.gamma)*primal_out.x.dot(mu)[0] # compute the optimal value of the primal\n",
    "abs(primal_value - dual_value) # print the absolute difference between the primal and dual optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467eaed",
   "metadata": {},
   "source": [
    "Note that the solution of the dual problem gives us the **optimal occupancy measure**. Given this:\n",
    "\n",
    "1. Obtain the **greedy policy** given the solution of the primal problem.\n",
    "\n",
    "2. **Evaluate** this policy and **compare** the obtained values with the solution of the dual problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36859e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(pi, env, tol=1e-10):\n",
    "    \"\"\"Implementation of policy evaluation through iteratively applying using a certain policy \n",
    "    Args:\n",
    "        pi: a policy\n",
    "        env: environment\n",
    "        tol: a scalar to dermerminate whether the policy evaluation convergences\n",
    "    Returns:\n",
    "        v: an array with the values of the actions chosen\n",
    "        q: an array with the q values    \n",
    "    \"\"\"\n",
    "    v = np.zeros(env.n_states)\n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    while True:\n",
    "        v_old = np.copy(v)\n",
    "        for a in range(env.n_actions):\n",
    "            q[:, a] = env.r[:, a] + env.gamma * env.sparseT[a].dot(v)\n",
    "        for s in range(env.n_states):\n",
    "            action_taken = pi[s]\n",
    "            v[s] = q[s,action_taken]\n",
    "        if np.linalg.norm(v - v_old) < tol:\n",
    "            break\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff915616",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = dual_out.x.reshape(gridworld.n_states,gridworld.n_actions).argmax(axis=1) # side remark/explanation: We know a deterministic optimal policy exists. It turns out that we can find it by checking for which component a lambda_opt(s,a) is > 0, and then setting pi_opt(a|s) = 1 for that action.\n",
    "v,q = evaluate_policy(pi,gridworld) # evaluate the policy\n",
    "print(np.linalg.norm(v-primal_out.x)) # print the 2-norm between the value function and the solution of the primal LP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e6075",
   "metadata": {},
   "source": [
    "***Questions (3 points)***\n",
    "\n",
    "Derive the dual from the primal. That is, show that it indeed the dual linear program of the primal program (and that thus the values coincide by strong duality).\n",
    "\n",
    "(*Hint:* You may consult the supplementary material of lecture 3, slide 9 and 10 in the appendix to read about strong duality in LPs. Make sure to explain step by step why it works.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca7485",
   "metadata": {},
   "source": [
    "***Answer:***\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f4964",
   "metadata": {},
   "source": [
    "# Ex 3: Implement REPS with known dynamics [39 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb495427-eda0-4c26-b105-cb6908cf3360",
   "metadata": {},
   "source": [
    "In this exercize you will **implement the REPS algorithm** as we have seen in the lectures.\n",
    "Namely, recall the following **details of the algorithm**:\n",
    "\n",
    "1. **Initialization** (uniform): $\\forall s, a, \\lambda_0(s,a) = \\frac{1}{|\\mathcal{S}||\\mathcal{A}|}$;\n",
    "\n",
    "2. **REPS loss computation**: $\\mathcal{L}(\\lambda, V) = (1-\\gamma) \\langle \\mu, V \\rangle + \\frac{1}{\\eta}\\log{\\langle \\lambda, \\exp{(\\eta(r+\\gamma P V - E V))} \\rangle}$;\n",
    "\n",
    "3. **Solving for the values**: $V_{k}=\\text{argmin}_{V}~{\\mathcal{L}(\\lambda_k, V)}$, for this we will use the `minimize` function from `scipy.optimize`;\n",
    "\n",
    "4. **Updating the occupancy measure:** $\\lambda_{k+1} \\propto \\lambda_k \\cdot \\exp{(\\eta (r+\\gamma P V_{k} - E V_{k}))}$ (unnormalized, you then must make sure it sums to 1 in every step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp, softmax\n",
    "from plot import plot_log_lines, plot_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da94e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reps_loss(lambda_, V, eta, initial): # return the reps loss\n",
    "    ???\n",
    "    return ???\n",
    "\n",
    "def minimize_reps_loss(lambda_, eta, initial): # find minimizer of the reps loss\n",
    "    loss = lambda V : reps_loss(lambda_, V, eta, initial)\n",
    "    V = minimize(loss, np.zeros(gridworld.n_states), method = \"CG\", options={'maxiter': 1000})\n",
    "    return V.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de14ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40 # number of iterations\n",
    "mu = np.ones((gridworld.n_states,1))/gridworld.n_states # initialize mu again to the uniform distribution (not needed, just to be safe)\n",
    "\n",
    "#initialize the variables\n",
    "subopts = []\n",
    "feasibility_errors = []\n",
    "lambda_ = np.ones(gridworld.n_states*gridworld.n_actions)/gridworld.n_states/gridworld.n_actions\n",
    "iterates = [lambda_]\n",
    "subopts.append(-lambda_.T.dot(gridworld.r.reshape(gridworld.n_states*gridworld.n_actions))+(1-gridworld.gamma)*mu.T.dot(solver.v)[0])\n",
    "feasibility_error = np.linalg.norm(build_matrix(gridworld).T.dot(lambda_) + (1-gridworld.gamma)*mu)\n",
    "feasibility_errors.append(feasibility_error)\n",
    "\n",
    "# REPS: iterate over K steps\n",
    "for k in range(K):\n",
    "    eta = 1/np.sqrt(k+1) # set the step size\n",
    "    \n",
    "    V = minimize_reps_loss(???) # minimize the reps loss\n",
    "    delta = gridworld.r.reshape(gridworld.n_states*gridworld.n_actions) + build_matrix(gridworld).dot(V) # compute the advantage function\n",
    "    \n",
    "    new_lambda_ = ??? # update the lambda\n",
    "    new_lambda_ /= np.sum(new_lambda_) # normalize the lambda_\n",
    "    lambda_ = new_lambda_\n",
    "    \n",
    "    # store the suboptimality and feasibility errors:\n",
    "    subopt = -lambda_.T.dot(gridworld.r.reshape(gridworld.n_states*gridworld.n_actions))+(1-gridworld.gamma)*mu.T.dot(solver.v)[0]\n",
    "    feasibility_error = np.linalg.norm(build_matrix(gridworld).T.dot(lambda_) + (1-gridworld.gamma)*mu)\n",
    "    subopts.append(subopt)\n",
    "    feasibility_errors.append(feasibility_error)\n",
    "    iterates.append(lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c936e",
   "metadata": {},
   "source": [
    "**Plot of the suboptimality of iterates produced by REPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines([np.array(subopts)], [r\"Subopt of $\\lambda^t$\"], [\"Iteration\", \"Subopt\"], \"figs\", \"subopts.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f1188",
   "metadata": {},
   "source": [
    "**Theoretical Derivation of REPS**\n",
    "\n",
    "Prove that the iterates of REPS are equivalent to the iterates produced by the following updates:\n",
    "\n",
    "$$ \\lambda_{k+1} = \\mathrm{argmax}_{\\lambda} \\left( \\langle \\lambda, r \\rangle - \\frac{1}{\\eta} \\left\\langle \\lambda,~ \\log\\left(\\frac{\\lambda}{\\lambda_k}\\right) \\right\\rangle \\right) \\quad \\text{s.t.} \\quad E^T \\lambda = \\gamma P^T \\lambda + (1 - \\gamma) \\mu. $$\n",
    "\n",
    "(*Hint:* Check the slides of lecture if you do not know where to start.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ec124",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf664",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d8297",
   "metadata": {},
   "source": [
    "**Plot the constraints violation**\n",
    "\n",
    "Plot the value of $||(E^T - \\gamma P^T)\\lambda - (1 - \\gamma) \\mu || $, varying the parameter `maxiter` of the `scipy.minimize` function.\n",
    "\n",
    "(To be clear: in the code above, modify this line ```minimize(loss, np.zeros(gridworld.n_states), method = \"CG\", options={'maxiter': 1000})``` and plot for varying `maxiter`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c11789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may use this cell to experiment with the parameter and keep the one above the same if it helps you to keep the overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines([np.array(feasibility_errors)], [r\"Constraints violation\"], [\"Iteration\", \"$|| (E^T - \\gamma P^T)\\lambda - (1 - \\gamma) \\mu ||$\"], \"figs\", \"constraints.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6f891",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a897960",
   "metadata": {},
   "source": [
    "Empirically, is the value of $||(E^T - \\gamma P^T)\\lambda - (1 - \\gamma) \\mu || $ higher for high or low values of `maxiter`?\n",
    "\n",
    "Explain why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e958bf4",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdf423",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83182d0",
   "metadata": {},
   "source": [
    "**Question:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8380c0",
   "metadata": {},
   "source": [
    "Strictly speaking, are the actual iterates $\\lambda_k$ in the implementation above valid occupancy measures? In other words, is $E^T \\lambda_k = \\gamma P^T \\lambda_k + (1 - \\gamma) \\mu$ strictly satisfied for all iterates? Explain why this is (not) the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee434266",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b750b8",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bbf2d7",
   "metadata": {},
   "source": [
    "**Plot of the suboptimality of the policies extracted from the $\\lambda^k$'s**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632524e9",
   "metadata": {},
   "source": [
    "*Extract the policies from the $\\lambda^k$-iterates:* $$\\pi_{\\lambda^k}(a |s) = \\frac{\\lambda_k(s,a)} {\\sum_{a\\in\\mathcal{A}}\\lambda_k(s,a)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eae099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policies_from_occ_measures(occ_measures): # extract the policies from the occupancy measures\n",
    "    policies = []\n",
    "    for occ_measure in occ_measures:\n",
    "        policy = np.zeros((gridworld.n_states, gridworld.n_actions)) # initialize the policy\n",
    "        occ_measure = occ_measure.reshape(gridworld.n_states, -1) # reshape the occupancy measure\n",
    "        states_occ_measure = occ_measure.sum(axis=1) # compute the occupancy measure over states (not state-action pairs)\n",
    "        for s in range(gridworld.n_states):\n",
    "            policy[s] = ??? # compute the policy\n",
    "        policies.append(policy)\n",
    "        \n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = extract_policies_from_occ_measures(iterates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d33fac",
   "metadata": {},
   "source": [
    "*Extract the occupancy measures from the policies:* $$ \\lambda_{\\pi_{\\lambda^k}}(s,a) = (1 - \\gamma)\\sum^{\\infty}_{t=0} \\gamma^t \\mathbb{P}[s_t=s,a_t=a | s_0 \\sim \\mu, \\pi_{\\lambda^k}].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_occ_measures_from_policies(policies): # extract the occupancy measures from the policies (we do not require you to follow this)\n",
    "    occ_measures = []\n",
    "    for policy in policies:\n",
    "        v = - (1 - gridworld.gamma)*(mu.repeat(gridworld.n_actions,axis=1)*policy).reshape(gridworld.n_states*gridworld.n_actions)\n",
    "        matrix = np.eye(gridworld.n_states*gridworld.n_actions)\n",
    "        T_pi = np.expand_dims(gridworld.T.transpose((1,0,2)),axis=3).repeat(gridworld.n_actions, axis=3)\n",
    "        pi_tensor = np.expand_dims(policy, axis=(0,1)).repeat(T_pi.shape[0], axis=0).repeat(T_pi.shape[1], axis=1)\n",
    "        T_pi = T_pi*pi_tensor\n",
    "        matrix = gridworld.gamma*T_pi.reshape(gridworld.n_states*gridworld.n_actions,gridworld.n_states*gridworld.n_actions)- matrix\n",
    "        occ_measure = np.linalg.solve(matrix.T, v)\n",
    "        occ_measures.append(occ_measure)\n",
    "    return occ_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_measures = extract_occ_measures_from_policies(policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7018c82",
   "metadata": {},
   "source": [
    "*Check whether $\\lambda_{\\pi_{\\lambda_k}} = \\lambda_k$*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa079032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the difference between the occupancy measures and the iterates\n",
    "diff_occ_measures = [np.linalg.norm(occ_measure - iterate) for occ_measure, iterate in zip(occ_measures, iterates)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e838a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the difference between the occupancy measures and the iterates\n",
    "plot_lines([np.array(diff_occ_measures), np.array(feasibility_errors)/(1 - gridworld.gamma)], [r\"Diff. Occupancy measures\", r\"$(1-\\gamma)^{-1}\\cdot$ Constraints violation\"], [\"Iteration\", \"$|| \\lambda - \\lambda_{\\pi_\\lambda}||$\"], \"figs\", \"diff_occ_measures.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868c099",
   "metadata": {},
   "source": [
    "**Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8c3bd",
   "metadata": {},
   "source": [
    "- How does the norm of the difference $|| \\lambda^k - \\lambda_{\\pi_{\\lambda^k}}||$ relate to the constraint violation $|| (E^T - \\gamma P^T)\\lambda^k - (1 - \\gamma) \\mu ||$ ?\n",
    "\n",
    "- If $|| (E^T - \\gamma P^T)\\lambda^k - (1 - \\gamma) \\mu ||$ is small, can you conclude that $|| \\lambda^k - \\lambda_{\\pi_{\\lambda^k}}||$ is also small ? \n",
    "\n",
    "*Hint: For both, you can answer leveraging the result in Lemma 3 of https://arxiv.org/pdf/2112.14004.pdf*\n",
    "\n",
    "- As a consequence how is $|| \\lambda^k - \\lambda_{\\pi_{\\lambda^k}}||$ affected by the value of `maxiter`?\n",
    "\n",
    "*Hint:* Recall your empirical finding about the impact of `maxiter` (above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd89588",
   "metadata": {},
   "source": [
    "***Answer:***\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f375e05",
   "metadata": {},
   "source": [
    "**Evaluate the extracted policies**\n",
    "\n",
    "In this section, we compute the value functions of the extracted policies which is $V^{\\pi_{\\lambda^k}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_sequence(policies, env, tol=1e-10):\n",
    "    values = []\n",
    "    for pi in policies:\n",
    "        v = np.zeros(env.n_states) # initialize value function\n",
    "        q = np.zeros((env.n_states, env.n_actions)) #initialize Q-value\n",
    "        while True:\n",
    "            v_old = np.copy(v) # save a copy of value function for the convergence criterion at the step\n",
    "            for a in range(env.n_actions):\n",
    "                q[:, a] = env.r[:, a] + env.gamma * env.sparseT[a].dot(v) #calculate Q-value\n",
    "            for s in range(env.n_states):\n",
    "                v[s] = pi[s].dot(q[s]) #calculate value function by $v(s) = max_a Q(s,a)$\n",
    "            if np.linalg.norm(v - v_old) < tol: # convergence criterion\n",
    "                break\n",
    "        values.append(v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correctness(policies):\n",
    "    values = evaluate_policy_sequence(policies, gridworld) # evaluate the policies\n",
    "    for policy,v in zip(policies,values):\n",
    "        state_occ_measure = solver.mu_policy(policy, stochastic=True) # compute the state occupancy measure\n",
    "        occ_measure = np.expand_dims(state_occ_measure, axis=1).repeat(gridworld.n_actions, axis=1)*policy\n",
    "        primal = occ_measure.reshape(gridworld.n_states*gridworld.n_actions).dot(gridworld.r.reshape(gridworld.n_states*gridworld.n_actions)) # compute the primal value\n",
    "        dual = (1 - gridworld.gamma)*mu.T.dot(v) # compute the dual value\n",
    "        assert primal - dual < 1e-7\n",
    "    return [ (1 - gridworld.gamma)*mu.T.dot(v) for v in values] # return the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4cbf7",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Do you expect $(1 - \\gamma)\\langle \\mu, V^{\\pi_{\\lambda^k}} \\rangle$ to be (approximately) equal to or (very) different from $\\langle \\lambda^k, r \\rangle $? Does the answer depend on the value of `maxiter`?\n",
    "\n",
    "*Hint: Recall that $(1 - \\gamma)\\langle \\mu, V^{\\pi_{\\lambda^k}} \\rangle = \\langle \\lambda_{\\pi_{\\lambda^k}}, r \\rangle $ and argue using your previous answer concerning the term $|| \\lambda^k - \\lambda_{\\pi_{\\lambda^k}}||$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf0be8",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d75030",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_values = check_correctness(policies) # check that $(1 - \\gamma)\\langle \\mu, V^{\\pi} \\rangle = \\langle \\lambda_{\\pi}, r \\rangle $ for all policies.\n",
    "policy_subopts = (1 - gridworld.gamma)*mu.T.dot(solver.v) - policy_values # compute the suboptimality of the policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5352e7c",
   "metadata": {},
   "source": [
    "**Plot the suboptimalities**\n",
    "\n",
    "Plot $(1 - \\gamma)\\langle \\mu, V^{\\pi^\\star}\\rangle - (1 - \\gamma)\\langle \\mu, V^{\\pi_{\\lambda^k}} \\rangle$.\n",
    "\n",
    "Plot $(1 - \\gamma)\\langle \\mu, V^{\\pi^\\star}\\rangle - \\langle \\lambda^k, r \\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines([np.array(subopts), policy_subopts.flatten()], [r\"Subopt of $\\lambda^t$\", r\"Subopt of $\\pi_{\\lambda^t}$\"], [\"Iteration\", \"Subopt\"], \"figs\", \"subopts.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8fde8",
   "metadata": {},
   "source": [
    "## Ex 4: Lagrangian methods [ 1 point, basically just an OPTIONAL exercise ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d5ca9",
   "metadata": {},
   "source": [
    "In this section we compute an optimal policy finding a saddle point of the Lagrangian $\\mathcal{L}(\\lambda, V)$, defined as follows\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\lambda, V) = \\langle \\lambda, r \\rangle + \\langle (1 - \\gamma) \\mu + \\gamma P^T \\lambda - E^T \\lambda, V \\rangle .\n",
    "$$\n",
    "\n",
    "Finding a saddle point of the Lagrangian means solving the following problem:\n",
    "\n",
    "$$ \n",
    "\\mathrm{argmax}_{\\lambda \\geq 0} \\min_{V \\in \\mathbb{R}^{|\\mathcal{S}|}} \\mathcal{L}(\\lambda, V).\n",
    "$$\n",
    "\n",
    "By slide 6 in Lecture 3 it should be clear that there exists a saddle point pair $\\lambda^\\star, V^\\star$ such that $|| V^\\star ||_{\\infty} \\leq \\frac{\\max_{s,a} |r(s,a)|}{1 - \\gamma}$. Therefore, we can consider looking for a saddle point over a restricted domain $\\mathcal{V} = \\{ V : ||V||_{\\infty} \\leq \\frac{\\max_{s,a} |r(s,a)|}{1 - \\gamma} \\}$:\n",
    "\n",
    "$$ \n",
    "\\mathrm{argmax}_{\\lambda \\geq 0} \\min_{V \\in \\mathcal{V}} \\mathcal{L}(\\lambda, V).\n",
    "$$\n",
    "\n",
    "At this point we solve this problem with gradient descent ascent updates \n",
    "\n",
    "$$\n",
    "\\lambda^{k+1} \\propto \\lambda^k \\odot \\exp(\\eta_{\\lambda} \\nabla_{\\lambda} \\mathcal{L}(\\lambda^k, V^k)),\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^{k+1} = \\Pi_{\\mathcal{V}}[V^k - \\eta_V \\nabla_{V} \\mathcal{L}(\\lambda^k, V^k)].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5827ef",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "\n",
    "Compute the following gradients:\n",
    "\n",
    "$$  \\nabla_{\\lambda} \\mathcal{L}(\\lambda^k, V^k) = ??? $$\n",
    "\n",
    "$$ \\nabla_{V} \\mathcal{L}(\\lambda^k, V^k) = ??? $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59955898",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Which of the following conditions ensure that the policy $$\\pi_{\\lambda^k}(a |s) = \\frac{\\lambda_k(s,a)} {\\sum_{a\\in\\mathcal{A}}\\lambda^k(s,a)}$$ is $\\epsilon$-suboptimal ?\n",
    "\n",
    "(a) $$\\langle \\lambda^\\star, r \\rangle - \\langle \\lambda^k, r \\rangle \\leq \\epsilon.$$\n",
    "(b) $$(1 - \\gamma)\\langle \\mu, V^\\star \\rangle - \\langle \\mu, V^k \\rangle \\leq \\epsilon.$$\n",
    "(c) $$ \\mathcal{L}(\\lambda^\\star, V^k) - \\mathcal{L}(\\lambda^k, V^{\\pi^{\\lambda^k}}) \\leq \\epsilon.$$\n",
    "\n",
    "Answer (a),(b) or (c) and explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b35d95",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e69ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a26741796d706ab2705a09fb3987e8aee731bdd9e4ce0df671a01d0db32297d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
